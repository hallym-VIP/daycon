PPT 작성시 반드시 확인
1) 날짜
2) 책 제목
3) 그림1 텍스트는 맑은 고딕(본문), 11pt

언어 모델이란 단어 시퀀스에  확률을 부여하는 모델이다.
시퀀스-투-시퀀스란, 특정 속성을 지닌 시퀀스를 다른 속성의 시퀀스로 변환하는 작업이다.
트랜스포머는 인코더와 디코더 입력이 주어졌을 때 정답에 해당하는 단어의 확률값을 높이는 방식으로 학습한다.
어텐션은 중요한요소에 더 집중해 성능을 끌어 올리는 기법이다.
셀프 어텐션은 쿼리, 키, 밸류가 서로 영향을 주고받으면서 문장의 의미를 계산한다.

프리트레인 - 사전학습
업스트림 태스크 : 다음 단어 맞추기, 빈칸 채우기 -> 결론적으로 업스트림 태스크가 뭐임?
다운스트림 태스크 : 분류 (자연어 처리의 구체적인 과제들)

텐서 : 벡터 공간

한국어를 영어로 바꾸는 중

쿼리, 키, 밸류를 만들어 주는 행렬이 존재함(Wq, Wk, Wv) 이걸 곱하면
이제 쿼리 값(벡터)Q, 키값K, 밸류값V이 나온다.

셀프어텐션을 동작할 때 쿼리 단어랑 가장 관련이 높은 키 단어는 자기자신이 될 수 없다.

트랜스포머의 학습 과제가 한국어에서 영어로 번역하는 태스크라면 인코더의 쿼리, 키, 밸류는 모두 한국어가 된다.(인코더!!!!)
->셀프 어텐션 수행 대상은 입력 시퀀스 전체(자기 자신)이기 때문이다.

디코더는 입력 시퀀스가 타깃 언어(영어로 바뀜) -> 쿼리, 키, 밸류는 모두 영어가 된다.

<s>는 번역 시작을 알리는 토큰이다. </s>는 문장 끝을 알리는 토큰

지금은 내가 가르쳐 주고 잘 공부했는지 확인하는 작업이다. 

ReLU : 정류 선형 유닛에 대한 함수. 입력 값이 0보다 작으면 0으로 출력, 0보다 크면 입력값 그대로 출력하는 유닛.
정류 : 전기공학에서 교류 전류를 직류로 바꾸는 것
선형 : 직선처럼 똑바른 도형, 또는 그와 비슷한 성질을 갖는 대상
유닛 : 단위(값)

입력층 : 입력 벡터가 자리잡는 층
은닉층 : 입력층과 출력층 사이에 위치하는 모든 층
출력층 : 최종 출력값이 자리잡는 층

잔차연결 : 그 길을 안가는게 아니라 그 길을 지나온 값과 안지나온 값을 나중에 만나서 둘이 더함
출발한 값 : X , 지나갈 함수 F(x), X가 함수를 지나간 값 : F(X) 
F(X) + X

미니 배치 : 데이터의 양이 굉장히 많은 경우에는 모든 데이터를 다 쓰는 것이 아니고, 데이터의 일부를 무작위로 추려서 그 근사치로 이용함. 
이 일부가 되는 데이터

파라미터==매개변수
오차==손실
오차를 최소화하는 방향 : 그레디언트
오차를 최소화하는 과정 : 최적화

미분의 연쇄법칙 : 합성함수의 미분 g(f(x))의 미분은 g'(y)f'(x)다

순전파 : 오차를 구하려고 모델 처음부터 끝까지 순서대로 계산해 보는 과정
역전파 : 순전파의 역순으로 순차적으로 수행
오차를 구했다면 오차를 최소화하는 최초긔 그레디언트를 구할 수 있다(미분)
미분의 연쇄법칙에 따라 모델 파라미터별 그레디언트 역시 구할 수 있음. 이 과정이 역전파로 진행

마스킹은 소프트맥스 확률이 0이 되도록 하여 밸류와의 가중합에서 해당 단어 정보들이 무시되게끔 하는 방식으로 수행

GRT(언어 모델) : 이전 단어들이 주어졌을 때 다음 단어가 무엇인지 맞히는 과정에서 프리트레인함. 
왼쪽에서 오른쪽으로 계산한다는 점에서 일방향 -> 문장 생성
트랜스포머에서 디코더만 취해 사용
앞 단어만 참고할 수 있기 때문에 정답 단어 이후의 모든 단어를 마스킹
이전 단어에 대응하는 GPT 마지막 레이어의 출력 결과에 선형 변환과 소프트맥스를 적용해 요솟값 각각이 확률이고 학습 대상 
언어의 어휘 수만큼 차원 수를 가진 벡터가 되도록 한다.



BERT(마스크 언어 모델) : 문장 중간에 빈칸을 만들고 해당 빈칸에 어떤 단어가 적절할지 맞히는 과정에서 프리트레인. 
빈칸 앞뒤 문맥을 모두 살필 수 있다는 점에서 양방향 -> 문장 의미 추출
트랜스포머에서 인코더만 취해 사용
마스크 단어에 대응하는 BERT 마지막 레이어의 출력 결과에 선형 변환과 소프트맥스를 적용해 요솟값 각각이 확률이고 학습 대상 
언어의 어휘 수만큼 차원 수를 가진 벡터가 되도록 한다


파인튜닝 : 프리트레인을마친 BERT와 그 위의 작은 모듈을 포함한 전체 모델을 문서 분류, 개체명 인식 등 다운스트림 데이터로 업데이트하는 과정

패딩 : 정해진 길이에 대해 길이보다 짧은 문장에는 숫자 0을 붙어서 길이를 맞춰줌(숫자 0을 사용하면 제로 패딩이라고 함)
길이가 전부 같으면 기계는 이들을 하나의 행렬로 보고, 병렬 처리를 할 수 있음

attention_mask가 0인 경우 해당 토큰에 대응하는 셀프 어텐션 소프트맥스 확률을 0으로 바꿈
-> 밸류와의 가중합에서 해당 단어 정보들이 무시되게끔 함

바이어스 : 입력된 신경망과 가중치 곱의 합이 가져야 할 최소 기준


-------------------------시작---------------------------------------

트랜스포머 모델에서 인코더와 디코더블록 부분입니다.
#멀티 헤드 어텐션은 앞 절에서 이미 살펴봤으므로 나머지 구성요소인 피드포워드 뉴럴 네트워크와 
#잔차 연결 및 레이어 정규화를 차례대로 살펴보겠습니다.

먼저 피드 포워드 뉴럴 넷 입니다.
x는 입력층, h는 은닉층, y는 출력층, b는 바이어스이고 그림2 에는 보이지 않지만 가중치 w가 존재합니다.
피드포워드 뉴럴 네트워크의 입력은 그림1에서 봤던 멀티헤드 어텐션의 개별 출력 벡터가 됩니다.
활성 함수 ReLU는 입력값이 양수이면 그대로 보내고 음수이면 0으로 치환합니다.
계산과정은 활성함수가 존재할 경우 수식1과 같으며 없을 경우에는 f를 제거한 것과 동일합니다.

잔차 연결이란 다음 그림처럼 블록이나 레이어 계산을 건너뛰는 경로를 하나 두는 것을 말합니다.
입력을 x, 이번 계산 대상 블록을 F라고 할 때 잔차 열결은 F(x) + x로 간단히 실현합니다.
잔차 연결의 장점으로 모델이 다양한 관점에서 블록 계산을 수행하게 되며, 모델 중간에 블록을 건너 뛰는 경로를 설정함으로써
학습을 쉽게 하는 효과까지 거둘 수 있습니다.
(ppt 내용이 너무 부족하다고 생각되면 p.107부분 그림 3-50 추가하자)

레이어 정규화
미니 배치의 인스턴스 별로 평균을 빼주고 표준편차로 나눠 정규화를 수행하는 기법입니다.
레이어 정규화를 수행하면 학습이 안정되고 그 속도가 빨라지는 효과가 있습니다.
베타와 감마는 학습 과정에서 업데이트되는 가중치이며 엡실론은 분모가 0이 되는 걸 방지하려고 더해주는 고정값인데 보통
1e-5로 설정합니다.
레이어 정규화하는 과정을 코랩으로 보면 이렇습니다.
출력 결과를 보면 감마를 곱하고 베타를 더해주는 과정이 생략된 것 같은 느낌이 드는데 이는
파이토치에서 두 값을 각각 1과 0으로 초기화 했기 때문입니다.

지금까지 트랜스포머 모델의 블록 부분을 살펴 봤다면 지금부터는 트랜스포머 모델의 학습 기법을 살펴보겠습니다.
딥러닝 모델은 표현력이 아주 좋아서 학습 데이터 그 자체를 외워버리는 과적합 상태가 될 수 있습니다. 이를 방지하고자 드롭아웃 기법을 활용합니다.
드롭아웃은 뉴런의 일부를 확률적으로 0으로 대치하여 계산에서 제외하는 기법입니다.
a는 일반적인 뉴럴 넷이고 b는 a에 드롭아웃을 적용한 예시입니다.

아담 옵티마이저를 설명하기전 순전파와 역전파에 대해 짚고 넘어가겠습니다.
(오차는 손실, 오차를 최소화 하는 방향은 그레디언트, 오차를 최소화하는 과정은 최적화)
순전파란 오차를 구하려고 모델 처음부터 끝까지 순서대로 계산해 보는 과정입니다. 오차를 구했다면 최초의 그레디언트를 구할 수 있는데 
이는 미분으로 구합니다. 이후 미분의 연쇄 법칙에 따라 모델 파라미터별 그레디언트를 구할 수 있습니다. 이 과정이 순전파의 역순으로 진행되어
역전파라고 부릅니다.
다시 돌아와서 모델 파라미터를 업데이트할 때 최적화 즉 오차를 최소화하는과정에서 사용되는 것이 아담 옵티마이저입니다.
책에서는 모델 파라미터 업데이트를 산등성이를 내려간다고 표현했습니다. 어느 방향으로 얼마만큼 보폭을 해야 더 빨리 산을 내려가는지 알려주는 것이 아담 옵티마이저라고 합니다.
(산을 올라가는 것은 이제 순전파, 역전파 과정에서 쓰는 도구가 아담 옵티마이저라고 생각함 나는)
(등산의 고지를 올라감 = 학습모델 생성, 하산하는데 빠르게 하산 하게 도와주는 것 = 옵티마이저)

3-5 장은 BERT와 GPT의 비교입니다.
GPT는 언어 모델입니다. 이전 단어들이 주어졌을 때 다음 단어가 무엇인지 밎히는 과정에서 프리트레인 합니다.
문장 왼쪽부터 오른쪽으로 순차적으로 계산한다는 점에서 일방향 입니다.
BERT는 마스크 언어 모델입니다. 문장 중간에 빈칸을 만들고 해당 빈칸에 어떤 단어가 적절할지 맞히는 과정에서 프리트레인 합니다.
빈칸 앞뒤 문맥을 모두 살필 수 있다는 점에서 양방향 성격을 가집니다.
그림 9는 GPT와 BERT의 프리트레인 방법입니다.

GPT는 트랜스포머에서 인코더를 제외하고 디코더만 사용하기 때문에 인코더 쪽에서 보내오는 정보를 받는 모듈(멀티 헤드 어텐션) 역시 제거돼 있습니다.
그림 11은 GPT의 셀프 어텐션입니다. 입력 단어 시퀀스에 대하여 이번에 카페를 맞혀야 하는 상황인데 이때 GPT는 어제라는 단어만 참고할 수 있습니다.
따라서 정답 단어ㅣ 이후의 모든 단어를 볼 수 없도록 처리합니다.
(밸류 벡터들을 가중합할 때 참고할 수 없는 단어에 곱하는 점수가 0이 되도록 합니다.)
'어제'라는 단어에 대응하는 GPT 마지막 레이어의 출력 결과에 선형 변환과 소프트맥스를 적용해 요솟값 각각이 확률이고 학습 대상 언어의 어휘 수만큼
차원 수를 가진 벡터가 되도록 합니다. 그리고 이번 차례의 정답인 카페에 해당하는 확률은 높이고 
나머지 단어의 확률은 낮아지도록 모델 전체를 업데이트 합니다.
(소프트맥스 : 입력받은 값을 출력으로 0~1사이의 값으로 모두 정규화하며 출력 값들의 총합은 항상 1이 되는 특성을 가진 함수)

BERT는 트랜스포머에서 디코더를 제외하고 인코더만 사용합니다.
그림 13은 BERT의 셀프 어텐션입니다. 입력 단어 시퀀스가 이렇다고 가정하면 BERT는 마스크 토큰 앞뒤 문맥을 모두 참고할 수 있습니다.
[MASK]라는 단어에 대응하는 BERT 마지막 레이어의 출력 결과에 선형 변환과 소프트맥스를 적용해 요솟값 각각이 확률이고 학습 대상 언어의
어휘수만큼 차원 수를 가진 벡터가 되도록 합니다. 빈칸의 정답인 '거기'에 해당하는 확률은 높이고 나머지 단어의 확률은
낮아지도록 모델 전체를 업데이트 합니다.

책에서 진행하는 실습은 모두 트랜스포머 계열 언어 모델을 사용합니다.
프리트레인을 마친 언어 모델 위에 작은 모듈을 조금 더 쌓아 태스크를 수행하는 구조입니다.
파인튜닝은~ppt읽기

(두 모델은 '빈칸 맞히기'로 프리트레인을 이미 마쳤다.)
문서 분류를 수행하는 모델은 그림 14와 같습니다. 그림에서 노란색 박스가 바로 BERT입니다.
(CLS와 SEP는 문장 시작과 긑을 알리는 스페셜 토큰)
워드피스로 토큰화한 후 앞뒤에 스페셜 토큰을 추가한 뒤 BERT에 입력합니다. 이후 마지막 블록의 출력 가운데 CLS에 해당하는 벡터를 추출합니다.
여기에 간단한 처리를 해서 그림에서는 나오지 않았는데 pooler_output 이라는 최종 출력을 만듭니다. 트랜스포머 인코더 블록에서는
모든 단어가 서로 영향을 끼치므로 문장 전체의 의미가 pooler_output 벡터 하나로 응집됩니다.
pooler_output 벡터에 작은 모듈을 하나 추가해 그 출력이 미리 정해 놓은 범주(예로 긍정, 중립, 부정)가 될 확률이 되도록 합니다.
학습 과정에서는 BERT와 그 위에 쌓은 작은 모듈을 포함한 전체 모델의 출력이 정답 레이블과 최대한 같아지도록 모델 전체를 업데이트 하는데
이것이 파인튜닝입니다.

그림 15도 마찬가지로 노란색 박스가 BERT모델입니다. 문서 분류 때와 같은 방식으로 마지막 레이어까지 계산을 수행합니다. 
BERT 모델의 마지막 블록(레이어)의 출력은 문장 내 모든 단어에 해당하는 벡터들의 시퀀스가 됩니다. 이렇게 뽑은 단어(토큰) 벡터들 위에
작은 모듈을 각각 추가해 그 출력이 각 개체명 범주(기관명, 인명, 지명)가 될 확률이 되도록 합니다. 학습 과정에서는 마찬가지로 파인튜닝을 합니다.
(학습 과정에서는 BERT와 그 위에 쌓은 작은 모듈을 포함한 전체 모델의 출력이 정답 레이블과 최대한 같아지도록 모델 전체를 업데이트합니다)

BertConfig {
  "_name_or_path": "beomi/kcbert-base", #이름or경로
  "architectures": [ 			   #뭐로 설계했는가
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,   #
  "classifier_dropout": null,		   #
  "directionality": "bidi",		   #방향성
  "gradient_checkpointing": false,	   #
  "hidden_act": "gelu",		   #
  "hidden_dropout_prob": 0.1,	   #
  "hidden_size": 768,		   #숨겨진 크기?
  "initializer_range": 0.02,		   #초기화 범위
  "intermediate_size": 3072,		   #중간 크기?
  "layer_norm_eps": 1e-12,		   #레이어 정규화 고정값?
  "max_position_embeddings": 300,	   #
  "model_type": "bert",		   #모델 종류
  "num_attention_heads": 12,		   #어텐션 헤드 수
  "num_hidden_layers": 12,		   #은닉층 수
  "pad_token_id": 0,		   #패딩 할때 토큰의 id. 0이니까 제로패딩인듯
  "pooler_fc_size": 768,		   #
  "pooler_num_attention_heads": 12,	   #
  "pooler_num_fc_layers": 3,                 #
  "pooler_size_per_head": 128,	   #
  "pooler_type": "first_token_transform",  #
  "position_embedding_type": "absolute",#
  "transformers_version": "4.10.0",	   #
  "type_vocab_size": 2,		   #
  "use_cache": true,			   #
  "vocab_size": 30000		   #어휘 집합 크기
}

문장을 벡터로 변환하는건 코랩을 통해 알아봤습니다.

모델 선언
모델 선언시 주의할 점은 BERT 모델이 프리트레인 할 때 썻던 토크나이저를 그대로 사용해야 벡터 변환에 문제가 없습니다.


BERT 모델의 입력값 만들기
문장 시작 CLS는 인덱스 2가, 문장 끝 SEP에는 인덱스 3이 붙었습니다.
토큰 최대 길이를 10으로 설정하고 이보다 짧으면 최대 길이에 맞게 패딩을 주고 길면 자르도록 설정했습니다.

#BERT로 단어/문장 수준 벡터 구하기
파이토치 모델의 입력값 자료형은 파이토치에서 제공하는 텐서여야 해서 텐서로 변환해주는 코드입니다.

#임베딩 계산하기
이 출력 결과는 문장 2개의 입력 토큰 각각에 해당하는 BERT의 마지막 레이어 출력 벡터들입니다.

그림 16은 '안녕하세요'만 따로 떼어서 계산 과정을 나타낸 것입니다.
outputs.last_hidden_states는 이 그림에서 주황색 점선으로 표기한 벡터들이 대응합니다.
(outputs.last_hidden_states = 단어 주순의 벡터 시퀀스)
(sentences의 입력 토큰 각각에 해당하는 BERT의 마지막 레이어 출력 벡터들)
이러한 결과는 개체명 인식 과제처럼 단어별로 수행해야 하는 태스크에 활용됩니다.
그림16에서 패딩에 해당하는 토큰들은 셀프 어텐션에서의 상호작용이 제한됩니다.
(해당 토큰의 attention_mask가 0이기 때문 ->해당 토큰에 대응하는 셀프 어텐션
소프트 맥스 확률을 0으로 바꿈->밸류와의 가중합에서 해당 단어 정보들이 무시되게끔 하는 방식으로 수행)

pooler_output을 만드는 과정은 그림 17과 같습니다.
마지막 레이어 h(CLS)벡터에 행렬 하나를 곱한 뒤 해당 벡터 요솟값 각각에 하이퍼볼릭탄젠트를 취합니다.
(하이퍼볼릭탄젠트 : 출력의 범위를 -1~1사이로 제한)
(pooler_output = 문장 수준의 벡터)

이처럼 자연어를 벡터로 바꾼 결과를 임베딩 또는 리프레젠테이션이라고 합니다.
문장을 단어 수준의 벡터 시퀀스로 변환하면 단어 수준 임베딩, 문장 수준의 벡터로 변환하면 문장 수준 임베딩이라고 부릅니다.


import torch
from ratsnlp.nlpbook.classification import ClassificationTrainArguments
args = ClassificationTrainArguments(
    pretrained_model_name="beomi/kcbert-base", #프리트레인 마친 언어 모델의 이름(단, 해당 모델은 허깅페이스 모델 허브에 등록되어 있어야 합니다.)

    downstream_corpus_name="nsmc", #다운스트림 데이터의 이름

    #downstream_corpus_root_dir : 다운스트림 데이터를 내려받을 위치. 입력하지 않으면 /root/Korpora 에 저장 됩니다.

    downstream_model_dir="/gdrive/My Drive/nlpbook/checkpoint-doccls", #파인튜닝된 모델의 체크포인트가 저장될 위치. 
    #/gdrive/My Drive/nlpbook/checkpoint-doccls로 지정하면 자신의 구글 드라이브 [내폴더] 아래 nlpbook.checkpoint-doccls디렉터리에 저장

    batch_size=32 if torch.cuda.is_available() else 4,  #배치크기. 하드웨어 가속기로 GPU를 선택(torch.cuda.is_available() == True)했다면 32, 
    #TPU라면(torch.cuda.is_available() == False) 4. 코랩 환경에서 TPU는 보통 8개 코거아 할당 되는데 batch_size는 코어별로 적용되는 배치 크기이므로 이렇게 설정해 둡니다.

    learning_rate=5e-5, #러닝 레이트(보폭). 1회 스텝에서 모델을 얼마나 업데이트할지에 관한 크기를 가리킵니다. 자세한 설명은 3-4절 '아담 옵티마이저' 참고

    max_seq_length=128, #토큰 기준 입력 문장 최대 길이. 이보다 긴 문장은 자르고 짧은 문장은 최개대 길이만큼 스페셜 토큰([PAD])을 붙여줍니다.

    epochs=3, #학습 에포크 수. 3이라면 학습 데이터 전체를 3회 반복 학습합니다.

    tpu_cores=0 if torch.cuda.is_available() else 8, #TPU 코어 수. 하드웨어 가속기로 GPU를 선택했다면 0, TPU라면 8

    seed=7, #랜덤 시드(정수). NONE을 입력하면 랜덤 시드를 고정하지 않습니다.
)


문장 쌍 분류란 문장 2개각 주어졌을 때 해당 문장 사이의 관계가 어떤 범주일지 분류하는 과제다.

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
머신 러닝 : 경험을 통해 자동으로 개선하는 컴퓨터 알고리즘의 연구

fNIRs : 생체조직에 대해 투과성이 높은 근적외광을 두피에 조사하여 조직을 투과해온 빛을 분석함으로써 조직을 흐르고 있는 혈액 중 헤모글로빈 산소화 상태를
비침슴적으로 계측하는 방법
	계측 : 측정을 포함하는 큰 개념으로 측정할 방법을 연구하고, 측정방법의 설계 등을 통해 측정의 형식을 정하고 그 측정으로 인해 얻는 결과값의 응용 및 적용

EEG : 전극을 통해 뇌의 전기적 활동을 기록하는 전기생리학적 층정방법


Bayesian Network : 조건부 확률을 사용하여 복잡한 모델을 쉽게 표현하기 위해 그래프로 표현하는 방식
